{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import gym \n",
    "import matplotlib\n",
    "import time\n",
    "import os\n",
    "\n",
    "from utils import Logger, MasterBuffer, Network\n",
    "from policy import Policy\n",
    "from density import DensityModel\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicsModel(object):\n",
    "    def __init__(self, graph_args, rollout_args):\n",
    "        self.enc_dim = graph_args['enc_dim']\n",
    "        self.act_dim = graph_args['act_dim']\n",
    "        self.act_low = graph_args['action_low']\n",
    "        self.act_high = graph_args['action_high']\n",
    "        \n",
    "        self.learning_rate = graph_args['learning_rate']\n",
    "        # network params\n",
    "        self.hid_size = graph_args['hid_size']\n",
    "        self.n_hidden = graph_args['n_hidden']\n",
    "        self.conv_depth = graph_args['conv_depth']\n",
    "        # rollout args\n",
    "        self.horizon = rollout_args['horizon']\n",
    "        self.num_rollouts = rollout_args['num_rollouts']\n",
    "        \n",
    "        self.state, self.action, self.n_state = self.setup_placeholders()\n",
    "        self.n_state_pred = self.dynamics_func(self.state, self.action, False)\n",
    "        \n",
    "        # Calculate Loss\n",
    "        delta = self.state - self.n_state\n",
    "        delta_pred = self.state - self.n_state_pred\n",
    "        self.loss = tf.losses.mean_squared_error(delta, delta_pred)\n",
    "        self.update_step = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
    "\n",
    "    def set_session(self, sess):\n",
    "        self.sess = sess\n",
    "\n",
    "    def setup_placeholders(self):\n",
    "        state = tf.placeholder(shape=(None, self.enc_dim), name=\"state\", dtype=tf.float32)\n",
    "        action = tf.placeholder(shape=(None, self.act_dim), name='action', dtype=tf.float32)\n",
    "        n_state = tf.placeholder(shape=(None, self.enc_dim), name=\"next_state\", dtype=tf.float32)\n",
    "        return state, action, n_state\n",
    "\n",
    "    def dynamics_func(self, state, action, reuse):\n",
    "        # add state, action normalization?\n",
    "        sa = tf.concat([state, action], axis=1)\n",
    "        delta_pred = Network(sa, self.enc_dim, 'dynamics', self.hid_size, conv_depth=0, n_hidden_dense=self.n_hidden, reuse=reuse)\n",
    "        n_state_pred = state + delta_pred\n",
    "        return n_state_pred    \n",
    "    \n",
    "    def update(self, state, action, n_state):\n",
    "        loss, _ = self.sess.run([self.loss, self.update_step], feed_dict={\n",
    "            self.state: state,\n",
    "            self.action: action,\n",
    "            self.n_state: n_state\n",
    "        })\n",
    "        return loss\n",
    "\n",
    "    def get_best_actions(self, state, profit_fcn, num_actions):\n",
    "        \"\"\" Given encoded state will return `num_rollouts` rollouts where each rollout is of size `horizon`.\n",
    "        Encoded state.\n",
    "        => rollouts \n",
    "        => apply profit_fcn to rollouts \n",
    "        => choose rollout with highest profit \n",
    "        => return (first k-actions of best rollout, profit of rollout)\n",
    "        \"\"\"\n",
    "        ran_sample = tf.random_uniform((self.horizon, self.num_rollouts, self.act_dim), \n",
    "                                       minval=self.act_low, \n",
    "                                       maxval=self.act_high, \n",
    "                                       dtype=tf.int32)\n",
    "        \n",
    "        rollout_profits = np.zeros((self.num_rollouts,), dtype=np.float32)\n",
    "        \n",
    "        # init state batch to starting state\n",
    "        state_batch = tf.ones((self.num_rollouts, 1), dtype=tf.float32) * state\n",
    "        for index in range(self.horizon):\n",
    "            act_batch = tf.cast(ran_sample[index], tf.float32)\n",
    "            next_state_batch = self.dynamics_func(state_batch, act_batch, True)\n",
    "            state_batch = next_state_batch\n",
    "            \n",
    "            # check profit of current states for all rollouts \n",
    "            profit_batch = profit_fcn(state_batch, evaluate=True, encoded=True)\n",
    "            rollout_profits += profit_batch\n",
    "            \n",
    "        max_profit_rollout = np.argmax(rollout_profits, axis=0)\n",
    "        ran_sample = tf.transpose(ran_sample, perm=(1, 0, 2))\n",
    "        best_actions = ran_sample[max_profit_rollout][:num_actions]\n",
    "        return best_actions, rollout_profits[max_profit_rollout]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # most params are default taken from homework\n",
    "# env = gym.make('MontezumaRevenge-v0')\n",
    "\n",
    "# ## STATE DENSITY FUNCTION PARAMS\n",
    "# density_graph_args = {\n",
    "#     'ob_dim': env.observation_space.shape,\n",
    "#     'learning_rate': 5e-3,\n",
    "#     'z_size': 32,\n",
    "#     'kl_weight': 1e-2,\n",
    "#     'conv_depth': 5,\n",
    "#     'hid_size': 32,\n",
    "#     'n_hidden': 2,\n",
    "#     'bonus_multiplier': 1\n",
    "# }\n",
    "\n",
    "# ## POLICY PARAMS\n",
    "# policy_graph_args = {\n",
    "#     'ob_dim': env.observation_space.shape,\n",
    "#     'act_dim': env.action_space.n,\n",
    "#     'clip_range': 0.2,\n",
    "#     'conv_depth': 5,\n",
    "#     'filter_size': 32,\n",
    "#     'learning_rate': 5e-3,\n",
    "#     'num_target_updates': 10,\n",
    "#     'num_grad_steps_per_target_update': 10\n",
    "# }\n",
    "# adv_args = {\n",
    "#     'gamma': 0.9999999\n",
    "# }\n",
    "\n",
    "# ## STATE DYNAMICS PARAMS\n",
    "# action_low = 0\n",
    "# action_high = env.action_space.n\n",
    "# dynamics_graph_args = {\n",
    "#     'ob_dim': env.observation_space.shape,\n",
    "#     'act_dim': env.action_space.n,\n",
    "#     'action_low': action_low,\n",
    "#     'action_high': action_high,\n",
    "#     'learning_rate': 1e-3,\n",
    "#     'hid_size': 250,\n",
    "#     'n_hidden': 2\n",
    "# }\n",
    "# dynamics_rollout_args = {\n",
    "#     'horizon': 30,\n",
    "#     'num_rollouts': 1000,\n",
    "# }\n",
    "\n",
    "# ## AGENT PARAMS\n",
    "# agent_args = {\n",
    "#     'density_train_itr': 200,\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.reset_default_graph()\n",
    "# # models\n",
    "# policy = Policy(policy_graph_args, adv_args)\n",
    "# density_model = DensityModel(density_graph_args)\n",
    "# dynamics_model = DynamicsModel(dynamics_graph_args, dynamics_rollout_args)\n",
    "# # utils # SmallRAMProblems\n",
    "# replay_buffer = MasterBuffer(max_size=40000)\n",
    "# logger = Logger()\n",
    "# # agent which will do all the work\n",
    "# agent = Agent(env, policy, density_model, dynamics_model, replay_buffer, logger, agent_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, env, policy, density, dynamics, replay_buffer, logger, args):\n",
    "        self.env = env\n",
    "        # Models\n",
    "        self.policy = policy\n",
    "        self.density = density\n",
    "        self.dynamics = dynamics\n",
    "        # Utils\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.logger = logger\n",
    "        # Args\n",
    "        self.density_train_itr = args['density_train_itr']\n",
    "        self.dynamics_train_itr = args['dynamics_train_itr']\n",
    "        self.num_actions_taken_conseq = args['num_actions_taken_conseq']\n",
    "        self.profit_fcn = self.density.get_bonus\n",
    "        \n",
    "    def set_session(self, sess):\n",
    "        self.sess = sess\n",
    "        self.policy.set_session(sess)\n",
    "        self.density.set_session(sess)\n",
    "        self.dynamics.set_session(sess)\n",
    "        \n",
    "    def sample_env(self, batch_size, num_samples, shuffle, action_selection):\n",
    "        obs = self.env.reset()\n",
    "        actions = []\n",
    "        taking_action = False\n",
    "        \n",
    "        for _ in range(num_samples):\n",
    "            if action_selection == 'random':\n",
    "                act = self.env.action_space.sample()\n",
    "            elif action_selection == 'algorithm':\n",
    "                if not taking_action:\n",
    "                    actions = self.get_action(obs, self.num_actions_taken_conseq)\n",
    "                    action_index, taking_action = 0, True\n",
    "                act = actions[action_index][0]\n",
    "                action_index += 1\n",
    "                # check for end of actions\n",
    "                if action_index == self.num_actions_taken_conseq: taking_action = False\n",
    "            # execute action and record\n",
    "            n_ob, rew, done, _ = env.step(act)\n",
    "            self.replay_buffer.record(obs, act, rew, n_ob, done)\n",
    "            obs = n_ob if not done else env.reset()\n",
    "        \n",
    "        # get logprobs of taking actions w.r.t current policy\n",
    "        obs, actions = self.replay_buffer.get_actions()\n",
    "        logprobs = sess.run(policy.logprob, feed_dict={\n",
    "            policy.obs: obs,\n",
    "            policy.act: actions\n",
    "        })\n",
    "        self.replay_buffer.set_logprobs(logprobs)\n",
    "        self.replay_buffer.merge_temp()\n",
    "        return self.replay_buffer.get_all(batch_size, master=False, shuffle=shuffle)\n",
    "        \n",
    "    def get_action(self, obs, num_actions):\n",
    "        # Encode observation from density encoder\n",
    "        enc_obs = self.density.get_encoding([obs])[0]\n",
    "        # get action from state dynamics\n",
    "        actions, profit = self.dynamics.get_best_actions(enc_obs, self.profit_fcn, num_actions)\n",
    "        # AFTER TESTING ====> if profit < eps:  actions = policy.get_actions()\n",
    "        logger.log('dynamics', ['max_profit'], [profit])\n",
    "        return actions.eval()\n",
    "        \n",
    "    def get_data(self, batch_size, num_samples, itr):\n",
    "        if itr < 20 or np.random.randint(10) < 6:\n",
    "            return self.sample_env(batch_size, num_samples, shuffle=True, action_selection='algorithm')\n",
    "        else:\n",
    "            return self.replay_buffer.get_all(batch_size, master=True, shuffle=True, size=num_samples)\n",
    "    \n",
    "    def train(self, batch_size, num_samples, itr):\n",
    "        obsList, actsList, rewardsList, n_obsList, donesList, logprobsList = self.get_data(batch_size, num_samples, itr)\n",
    "        self.replay_buffer.flush_temp()\n",
    "        \n",
    "        # process in batches of data\n",
    "        for obs, acts, rewards, n_obs, dones, logprobs in zip(obsList, actsList, rewardsList, n_obsList, donesList, logprobsList):\n",
    "            # train density model\n",
    "            for _ in range(self.density_train_itr):\n",
    "                s1, s2, target = self.replay_buffer.get_density_batch(obs, batch_size)\n",
    "                ll, kl, elbo = self.density.update(s1, s2, target)\n",
    "                self.logger.log('density', ['loss', 'kl', 'elbo'], [ll, kl, elbo])\n",
    "                \n",
    "            # update dynamics \n",
    "            for _ in range(self.dynamics_train_itr):\n",
    "                # encode states\n",
    "                enc_obs = self.density.get_encoding(obs)\n",
    "                enc_n_obs = self.density.get_encoding(n_obs)\n",
    "                loss = self.dynamics.update(enc_obs, acts, enc_n_obs)\n",
    "                self.logger.log('dynamics', ['loss'], [loss])\n",
    "            \n",
    "            # train critic & actor\n",
    "            # inject exploration bonus\n",
    "#             rewards = self.density.modify_reward(obs, rewards)\n",
    "#             critic_loss = self.policy.train_critic(obs, n_obs, rewards, dones)\n",
    "#             adv = self.policy.estimate_adv(obs, rewards, n_obs, dones)\n",
    "#             actor_loss = self.policy.train_actor(obs, acts, logprobs, adv)\n",
    "#             self.logger.log('policy', ['actor_loss', 'critic_loss'], [actor_loss, critic_loss])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test State Dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/gym/envs/registration.py:14: DeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MontezumaRevenge-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to train...\n",
      "(array([[ 1],\n",
      "       [12],\n",
      "       [11],\n",
      "       [ 8],\n",
      "       [ 0]], dtype=int32), 40.50349)\n",
      "(array([[16],\n",
      "       [ 5],\n",
      "       [14],\n",
      "       [12],\n",
      "       [12]], dtype=int32), 33.02115)\n",
      "(array([[ 2],\n",
      "       [ 5],\n",
      "       [ 8],\n",
      "       [ 5],\n",
      "       [12]], dtype=int32), 39.36689)\n",
      "(array([[ 0],\n",
      "       [10],\n",
      "       [ 4],\n",
      "       [ 3],\n",
      "       [ 1]], dtype=int32), 22.812048)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-05eedcfbf815>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mitr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mitr\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mrender_n\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-66-df3210c7ded3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, batch_size, num_samples, itr)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mobsList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactsList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewardsList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_obsList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdonesList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogprobsList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush_temp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-66-df3210c7ded3>\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(self, batch_size, num_samples, itr)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mitr\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m20\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_selection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'algorithm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaster\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-66-df3210c7ded3>\u001b[0m in \u001b[0;36msample_env\u001b[0;34m(self, batch_size, num_samples, shuffle, action_selection)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0maction_selection\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'algorithm'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtaking_action\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                     \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_actions_taken_conseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m                     \u001b[0maction_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtaking_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0mact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-66-df3210c7ded3>\u001b[0m in \u001b[0;36mget_action\u001b[0;34m(self, obs, num_actions)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0menc_obs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdensity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m# get action from state dynamics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprofit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdynamics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofit_fcn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprofit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;31m# AFTER TESTING ====> if profit < eps:  actions = policy.get_actions()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-62-bb9e4c606d98>\u001b[0m in \u001b[0;36mget_best_actions\u001b[0;34m(self, state, profit_fcn, num_actions)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;31m# check profit of current states for all rollouts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0mprofit_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprofit_fcn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0mrollout_profits\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mprofit_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/brennangebotys/Documents/workspace/PredictiveExploration/codebase/density.py\u001b[0m in \u001b[0;36mget_bonus\u001b[0;34m(self, states, evaluate, encoded)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_bonus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m         \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m     \"\"\"\n\u001b[0;32m--> 713\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[0;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   5155\u001b[0m                        \u001b[0;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5156\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 5157\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "# STATE DYNAMICS PARAMS\n",
    "action_low = 0\n",
    "action_high = env.action_space.n\n",
    "dynamics_graph_args = {\n",
    "    'enc_dim': density_graph_args['z_size'],\n",
    "    'act_dim': 1,\n",
    "    'action_low': action_low,\n",
    "    'action_high': action_high,\n",
    "    'learning_rate': 1e-3,\n",
    "    'hid_size': 64,\n",
    "    'conv_depth': 4,\n",
    "    'n_hidden': 2\n",
    "}\n",
    "dynamics_rollout_args = {\n",
    "    'horizon': 30,\n",
    "    'num_rollouts': 1000,\n",
    "}\n",
    "\n",
    "## POLICY PARAMS\n",
    "policy_graph_args = {\n",
    "    'ob_dim': env.observation_space.shape,\n",
    "    'act_dim': env.action_space.n,\n",
    "    'clip_range': 0.2,\n",
    "    'conv_depth': 5,\n",
    "    'filter_size': 32,\n",
    "    'learning_rate': 5e-3,\n",
    "    'num_target_updates': 10,\n",
    "    'num_grad_steps_per_target_update': 10\n",
    "}\n",
    "adv_args = {\n",
    "    'gamma': 0.9999999\n",
    "}\n",
    "\n",
    "## STATE DENSITY FUNCTION PARAMS\n",
    "density_graph_args = {\n",
    "    'ob_dim': env.observation_space.shape,\n",
    "    'learning_rate': 5e-3,\n",
    "    'z_size': 32,\n",
    "    'kl_weight': 1e-2,\n",
    "    'conv_depth': 5,\n",
    "    'hid_size': 32,\n",
    "    'n_hidden': 2,\n",
    "    'bonus_multiplier': 1\n",
    "}\n",
    "## AGENT PARAMS\n",
    "agent_args = {\n",
    "    'density_train_itr': 200,\n",
    "    'dynamics_train_itr': 5,\n",
    "    'num_actions_taken_conseq': 5,\n",
    "}\n",
    "\n",
    "policy = Policy(policy_graph_args, adv_args)\n",
    "dynamics = DynamicsModel(dynamics_graph_args, dynamics_rollout_args)\n",
    "density = DensityModel(density_graph_args)\n",
    "\n",
    "replay_buffer = MasterBuffer(max_size=40000)\n",
    "logger = Logger()\n",
    "agent = Agent(env, policy, density, dynamics, replay_buffer, logger, agent_args)\n",
    "\n",
    "tf_config = tf.ConfigProto(inter_op_parallelism_threads=1, intra_op_parallelism_threads=1)\n",
    "tf_config.gpu_options.allow_growth = True\n",
    "with tf.Session(config=tf_config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    agent.set_session(sess)\n",
    "    \n",
    "    n_iter = 51\n",
    "    num_samples = 256\n",
    "    batch_size = [32, 64, 128]\n",
    "    render_n = 10\n",
    "    \n",
    "    print('starting to train...')\n",
    "    for bs in batch_size:\n",
    "        for itr in range(n_iter):\n",
    "            agent.train(bs, num_samples, itr)\n",
    "            if itr % render_n == 0:\n",
    "                logger.export()\n",
    "                print('Exported logs...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viewing Party"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = list(Path('iter-frames').iterdir())\n",
    "logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_frames = 50\n",
    "for fn in reversed(logs):\n",
    "    if os.path.isfile(fn):\n",
    "        with open(fn, 'rb') as file:\n",
    "            frames = pickle.load(file)\n",
    "    #         frames, rewards = data[0], data[1]\n",
    "            # View \n",
    "            img = plt.imshow(frames[0])\n",
    "            plt.title('Iteration:' + str(fn))\n",
    "            for i, frame in enumerate(frames):\n",
    "                img.set_data(frame) # just update the data\n",
    "                display.display(plt.gcf())\n",
    "                display.clear_output(wait=True)\n",
    "                if i > n_frames: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones = tf.placeholder(shape=(5,5), dtype=tf.int32)\n",
    "twos = ones + 1\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    my_ones = tf.ones((5, 5), dtype=tf.int32)\n",
    "    my_twos = sess.run(twos, feed_dict={\n",
    "        ones: my_ones.eval()\n",
    "    })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
