{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np \n",
    "import copy\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import gym \n",
    "\n",
    "# rendering gym env\n",
    "from IPython import display\n",
    "import matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Network(input_tensor, output_size, scope, fsize, conv_depth, n_hidden_dense=0, activation=tf.tanh, output_activation=None):\n",
    "        with tf.variable_scope(scope):\n",
    "            x = input_tensor\n",
    "            # Convolutions\n",
    "            for _ in range(conv_depth):\n",
    "                x = tf.layers.conv2d(x, fsize, (3, 3), activation='relu')\n",
    "                x = tf.layers.conv2d(x, fsize, (3, 3), strides=(2, 2))\n",
    "            \n",
    "            # Dense Layers\n",
    "            x = tf.layers.flatten(x)\n",
    "            for _ in range(n_hidden_dense):\n",
    "                x = tf.layers.dense(x, fsize, activation=activation)\n",
    "            y = tf.layers.dense(x, output_size, activation=output_activation)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Policy(object):\n",
    "    def __init__(self, graph_args, adv_args):\n",
    "        self.ob_dim = graph_args['ob_dim']\n",
    "        self.act_dim = graph_args['act_dim']\n",
    "        clip_range = graph_args['clip_range']\n",
    "        # conv operations params\n",
    "        conv_depth = graph_args['conv_depth']\n",
    "        filter_size = graph_args['filter_size']\n",
    "        \n",
    "        self.learning_rate = graph_args['learning_rate']\n",
    "        self.num_target_updates = graph_args['num_target_updates']\n",
    "        self.num_grad_steps_per_target_update = graph_args['num_grad_steps_per_target_update']\n",
    "        \n",
    "        self.gamma = adv_args['gamma']\n",
    "        \n",
    "        self.obs, self.act, self.adv, self.old_logprob = self.define_placeholders()\n",
    "        \n",
    "        # policy / actor evaluation\n",
    "        self.policy_distrib = Network(self.obs, self.act_dim, 'policy', filter_size, conv_depth)\n",
    "        self.greedy_action = tf.argmax(self.policy_distrib, axis=1)\n",
    "        self.logprob = self.get_logprob(self.policy_distrib, self.act)\n",
    "        \n",
    "        # importance sampling\n",
    "        ratio = tf.exp(self.logprob - self.old_logprob)\n",
    "        clipped_ratio = tf.clip_by_value(ratio, 1.0-clip_range, 1.0+clip_range)        \n",
    "        # include increase entropy term with alpha=0.2\n",
    "        batch_loss = tf.minimum(ratio*self.adv, clipped_ratio * self.adv) - 0.2 * self.logprob\n",
    "        self.actor_loss = -1 * tf.reduce_mean(batch_loss)\n",
    "        self.actor_update_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.actor_loss)\n",
    "        \n",
    "        # critic definition\n",
    "        self.v_pred = tf.squeeze(Network(self.obs, 1, 'critic', filter_size, conv_depth, n_hidden_dense=2))\n",
    "        self.v_target = tf.placeholder(shape=(None,), name='v_target', dtype=tf.float32)\n",
    "        self.critic_loss = tf.losses.mean_squared_error(self.v_target, self.v_pred)\n",
    "        # minimize with respect to correct variables HERE\n",
    "        self.critic_update_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.critic_loss)\n",
    "        \n",
    "    def set_session(self, sess):\n",
    "        self.sess = sess\n",
    "        \n",
    "    def define_placeholders(self):\n",
    "        obs = tf.placeholder(shape=((None,) + (self.ob_dim)), name='obs', dtype=tf.float32)\n",
    "        act = tf.placeholder(shape=(None,), name='act', dtype=tf.int32)\n",
    "        adv = tf.placeholder(shape=(None,), name='adv', dtype=tf.float32)\n",
    "        logprob = tf.placeholder(shape=(None,), name='logprob', dtype=tf.float32)\n",
    "        return obs, act, adv, logprob\n",
    "    \n",
    "    def get_logprob(self, policy_distribution, actions):\n",
    "        action_enc = tf.one_hot(actions, depth=self.act_dim)\n",
    "        logprob = -1 * tf.nn.softmax_cross_entropy_with_logits_v2(logits=policy_distribution, labels=action_enc)\n",
    "        return logprob\n",
    "        \n",
    "    def get_best_action(self, observation):\n",
    "        act = sess.run(self.greedy_action, feed_dict={\n",
    "            self.obs: [observation]\n",
    "        })[0]\n",
    "        return act\n",
    "    \n",
    "    def estimate_adv(self, obs, rew, nxt_obs, dones):\n",
    "        ## Markov Implementation??\n",
    "        # V(s) & V(s')\n",
    "        v_obs = self.sess.run(self.v_pred, feed_dict={self.obs: obs})\n",
    "        v_nxt_obs = self.sess.run(self.v_pred, feed_dict={self.obs: nxt_obs})\n",
    "        # y = r + gamma * V(s')\n",
    "        y_obs = rew + (1 - dones) * self.gamma * v_nxt_obs\n",
    "        # Adv(s) = y - V(s)\n",
    "        adv = y_obs - v_obs\n",
    "        # Normalize advantages\n",
    "        adv = (adv - np.mean(adv)) / (np.std(adv) + 1e-8)\n",
    "        return adv\n",
    "    \n",
    "    def train_actor(self, obs, act, logprob, adv):\n",
    "        self.sess.run(self.actor_update_op, feed_dict={\n",
    "            self.obs: obs,\n",
    "            self.act: act,\n",
    "            self.adv: adv,\n",
    "            self.old_logprob: logprob\n",
    "        })\n",
    "        \n",
    "    def train_critic(self, obs, nxt_obs, rew, dones):\n",
    "        for i in range(self.num_grad_steps_per_target_update * self.num_target_updates):\n",
    "            if i % self.num_grad_steps_per_target_update == 0:\n",
    "                v_pred = self.sess.run(self.v_pred, feed_dict={self.obs: nxt_obs})\n",
    "                y = rew + self.gamma * v_pred * (1 - dones)\n",
    "                \n",
    "            _, loss = self.sess.run([self.critic_update_op, self.critic_loss],\n",
    "                                    feed_dict={self.obs: obs, self.v_target: y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, max_size=10000):\n",
    "        self.obs = []\n",
    "        self.acts = []\n",
    "        self.rewards = []\n",
    "        self.nxt_obs = []\n",
    "        self.dones = []\n",
    "        self.logprobs = []\n",
    "        self.max_size = max_size\n",
    "    \n",
    "    def record(self, obs, act, rew, nxt_ob, done):\n",
    "        self.obs.append(obs)\n",
    "        self.acts.append(act)\n",
    "        self.rewards.append(rew)\n",
    "        self.nxt_obs.append(nxt_ob)\n",
    "        self.dones.append(done)\n",
    "        \n",
    "    def get_actions(self):\n",
    "        return np.asarray(self.obs), np.asarray(self.acts)\n",
    "    \n",
    "    def set_logprobs(self, logprobs):\n",
    "        self.logprobs += list(logprobs)\n",
    "        assert len(self.logprobs) == len(self.obs), 'logprobs MUST == self.obs'\n",
    "        \n",
    "    def merge(self, obs, acts, rews, nxt_obs, dones, logprobs):\n",
    "        self.obs += obs\n",
    "        self.acts += acts\n",
    "        self.rewards += rews\n",
    "        self.nxt_obs += nxt_obs\n",
    "        self.dones += dones\n",
    "        self.logprobs += list(logprobs)\n",
    "    \n",
    "    def export(self):\n",
    "        return self.obs, self.acts, self.rewards, self.nxt_obs, self.dones, self.logprobs\n",
    "    \n",
    "    def get_samples(self, indices):\n",
    "        return (\n",
    "            np.asarray(self.obs)[indices],\n",
    "            np.asarray(self.acts)[indices],\n",
    "            np.asarray(self.rewards)[indices],\n",
    "            np.asarray(self.nxt_obs)[indices],\n",
    "            np.asarray(self.dones)[indices],\n",
    "            np.asarray(self.logprobs)[indices]\n",
    "        )\n",
    "\n",
    "    def get_all(self, batch_size):\n",
    "        batched_dsets = []\n",
    "        # batch up data\n",
    "        for dset in [np.asarray(self.obs), np.asarray(self.acts), np.asarray(self.rewards), np.asarray(self.nxt_obs), np.asarray(self.dones), np.asarray(self.logprobs)]:\n",
    "            bdset = []\n",
    "            for i in range(0, len(dset), batch_size):\n",
    "                 bdset.append(np.array(dset[i:i+batch_size]))\n",
    "            batched_dsets.append(np.asarray(bdset))\n",
    "        return tuple(batched_dsets)\n",
    "    \n",
    "    def update_size(self):\n",
    "        diff = self.max_size - len(self)\n",
    "        if diff < 0:\n",
    "            # FIFO\n",
    "            self.obs = self.obs[-diff:]\n",
    "            self.acts = self.acts[-diff:]\n",
    "            self.rewards = self.rewards[-diff:]\n",
    "            self.nxt_obs = self.acts[-diff:]\n",
    "            self.dones = self.dones[-diff:]\n",
    "            self.logprobs = self.logprobs[-diff:]\n",
    "    \n",
    "    def flush(self):\n",
    "        self.obs = []\n",
    "        self.acts = []\n",
    "        self.rewards = []\n",
    "        self.nxt_obs = []\n",
    "        self.dones = []\n",
    "        self.logprobs = []\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger(object):\n",
    "    def __init__(self):\n",
    "        self.tag = None\n",
    "        self.totalr = []\n",
    "        self.std_reward = []\n",
    "        \n",
    "        self.tags = []\n",
    "        self.results = []\n",
    "        \n",
    "        self.fset = Path('iter-frames')\n",
    "        self.fset.mkdir(exist_ok=True)\n",
    "        self.n_frames_stored = 0\n",
    "        \n",
    "    def set_tag(self, tag):\n",
    "        self.tag = tag\n",
    "        \n",
    "    def log(self, totalr, std):\n",
    "        self.totalr.append(totalr)\n",
    "        self.std_reward.append(std)\n",
    "        \n",
    "    def log_frames(self, frames):\n",
    "        fname = '{}'.format(self.n_frames_stored)\n",
    "        with open(self.fset/fname, 'wb') as f:\n",
    "            pickle.dump(frames, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "        self.n_frames_stored += 1\n",
    "        \n",
    "    def package_results(self):\n",
    "        # store\n",
    "        self.tags.append(self.tag)\n",
    "        self.results.append([self.totalr, self.std_reward])\n",
    "    \n",
    "    def flush(self):\n",
    "        self.totalr = []\n",
    "        self.std_reward = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Actor Critic Test On Cart-Pole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for results in logger.results:\n",
    "#     totalr, stdr = results[0], results[1]\n",
    "#     plt.plot(totalr)\n",
    "# plt.title('AvgTotalReward vs Train Iteration')\n",
    "# plt.xlabel('TrainIteration')\n",
    "# plt.ylabel('AvgTotalReward')\n",
    "# plt.legend(logger.tags, loc='upper left')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Density Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DensityModel(object):\n",
    "    def __init__(self, graph_args):\n",
    "        # unpacking\n",
    "        self.ob_dim = graph_args['ob_dim']\n",
    "        self.learning_rate = graph_args['learning_rate']\n",
    "        self.z_size = graph_args['z_size']\n",
    "        self.kl_weight = graph_args['kl_weight']\n",
    "        # network operations params\n",
    "        self.hid_size = graph_args['hid_size']\n",
    "        self.n_hidden = graph_args['n_hidden']\n",
    "        \n",
    "        self.state1, self.state2 = self.define_placeholders()\n",
    "        # q(z_1 | s_1), q(z_2 | s_2), p(z), p(y | z)\n",
    "        self.encoder1, self.encoder2, self.prior, self.discriminator = self.forward_pass(self.state1, self.state2)\n",
    "        self.discrim_target = tf.placeholder(shape=[None, 1], name=\"discrim_target\", dtype=tf.float32)\n",
    "\n",
    "        self.log_likelihood = tf.squeeze(self.discriminator.log_prob(self.discrim_target), axis=1)\n",
    "        self.likelihood = tf.squeeze(self.discriminator.prob(self.discrim_target), axis=1)\n",
    "        \n",
    "        self.kl = self.encoder1.kl_divergence(self.prior) + self.encoder2.kl_divergence(self.prior)\n",
    "\n",
    "        assert len(self.log_likelihood.shape) == len(self.likelihood.shape) == len(self.kl.shape) == 1\n",
    "        \n",
    "        self.elbo = tf.reduce_mean(self.log_likelihood - self.kl_weight * self.kl)\n",
    "        self.update_op = tf.train.AdamOptimizer(self.learning_rate).minimize(-self.elbo)\n",
    "\n",
    "    def define_placeholders(self):\n",
    "        state1 = tf.placeholder(shape=((None,) + (self.ob_dim)), name=\"s1\", dtype=tf.float32)\n",
    "        state2 = tf.placeholder(shape=((None,) + (self.ob_dim)), name=\"s2\", dtype=tf.float32)\n",
    "        return state1, state2\n",
    "    \n",
    "    def set_session(self, sess):\n",
    "        self.sess = sess\n",
    "\n",
    "    #Network(input_tensor, output_size, scope, fsize, conv_depth, n_hidden_dense=0, activation=tf.tanh, output_activation=None):\n",
    "    def make_encoder(self, state, z_size, scope):\n",
    "        \"\"\" Encodes the given state to z_size => create guass. distribution for q(z | s)\n",
    "        \"\"\"\n",
    "        # conv operations\n",
    "        z_mean = Network(state, z_size, scope, self.hid_size, conv_depth=self.n_hidden)\n",
    "        z_logstd = tf.get_variable(\"logstd\", shape=(z_size,)) \n",
    "        return tfp.distributions.MultivariateNormalDiag(loc=z_mean, scale_diag=tf.exp(z_logstd))\n",
    "\n",
    "    def make_prior(self, z_size):\n",
    "        \"\"\" Create Prior to map states too => p(z), we will use a normal guass distrib\n",
    "        \"\"\"\n",
    "        prior_mean = tf.zeros((z_size,))\n",
    "        prior_logstd = tf.zeros((z_size,))\n",
    "        return tfp.distributions.MultivariateNormalDiag(loc=prior_mean, scale_diag=tf.exp(prior_logstd))\n",
    "\n",
    "    def make_discriminator(self, z, output_size, scope, n_layers, hid_size):\n",
    "        \"\"\" Predict D(z = [z1, z2]) => p(y | z)\n",
    "        \"\"\"\n",
    "        logit = Network(z, output_size, scope, hid_size, conv_depth=0, n_hidden_dense=n_layers)\n",
    "        return tfp.distributions.Bernoulli(logit)\n",
    "\n",
    "    def forward_pass(self, state1, state2):\n",
    "        # Reuse\n",
    "        make_encoder1 = tf.make_template('encoder1', self.make_encoder)\n",
    "        make_encoder2 = tf.make_template('encoder2', self.make_encoder)\n",
    "        make_discriminator = tf.make_template('decoder', self.make_discriminator)\n",
    "\n",
    "        # Encoder\n",
    "        encoder1 = make_encoder1(state1, self.z_size, 'z1')\n",
    "        encoder2 = make_encoder2(state2, self.z_size, 'z2')\n",
    "\n",
    "        # Prior\n",
    "        prior = self.make_prior(self.z_size)\n",
    "\n",
    "        # Sampled Latent (some noise)\n",
    "        self.z1 = encoder1.sample()\n",
    "        z2 = encoder2.sample()\n",
    "        z = tf.concat([self.z1, z2], axis=1)\n",
    "\n",
    "        # Discriminator\n",
    "        discriminator = make_discriminator(z, 1, 'discriminator', self.n_hidden, self.hid_size)\n",
    "        return encoder1, encoder2, prior, discriminator\n",
    "\n",
    "    def update(self, state1, state2, target):\n",
    "        _, ll, kl, elbo = self.sess.run([self.update_op, self.log_likelihood, self.kl, self.elbo], feed_dict={\n",
    "            self.state1: state1,\n",
    "            self.state2: state2,\n",
    "            self.discrim_target: target\n",
    "        })\n",
    "        return ll, kl, elbo\n",
    "    \n",
    "    def get_encoding(self, state):\n",
    "        \"\"\"Assuming only encode a single state at a time\n",
    "        We will call this to use in our state dynamics fcn\n",
    "        \"\"\"\n",
    "        return self.sess.run(self.z1, feed_dict={\n",
    "            self.state1: [state]\n",
    "        })[0]\n",
    "\n",
    "    def get_likelihood(self, state1, state2):\n",
    "        bs, _, _, _ = state1.shape\n",
    "        target = np.zeros((bs, 1))\n",
    "        for i, (s1, s2) in enumerate(zip(state1, state2)):\n",
    "            if s1.all() == s2.all(): target[i] = [1]\n",
    "\n",
    "        likelihood = self.sess.run(self.likelihood, feed_dict={\n",
    "            self.state1: state1,\n",
    "            self.state2: state2,\n",
    "            self.discrim_target: target\n",
    "        })\n",
    "        return likelihood\n",
    "\n",
    "    def get_prob(self, state):\n",
    "        likelihood = self.get_likelihood(state, state)\n",
    "        # avoid divide by 0 and log(0)\n",
    "        likelihood = np.clip(np.squeeze(likelihood), 1e-5, 1-1e-5)\n",
    "        prob = (1 - likelihood) / likelihood\n",
    "        return prob\n",
    "    \n",
    "    def modify_reward(self, state, rewards):\n",
    "        probs = self.get_prob(state)\n",
    "        bonus = -np.log(probs)\n",
    "        return rewards + 1e-3 * bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, env, policy, density, replay_buffer, logger):\n",
    "        self.env = env\n",
    "        self.policy = policy\n",
    "        self.density = density\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.logger = logger\n",
    "        \n",
    "    def set_session(self, sess):\n",
    "        self.sess = sess\n",
    "        self.policy.set_session(sess)\n",
    "        self.density.set_session(sess)\n",
    "        \n",
    "    def sample_env(self, num_samples):\n",
    "        obs = self.env.reset()\n",
    "        i = 0\n",
    "        while True:\n",
    "            act = self.choose_action(obs)\n",
    "            nxt_ob, rew, done, _ = env.step(act)\n",
    "            \n",
    "            replay_buffer.record(obs, act, rew, nxt_ob, done)\n",
    "            obs = nxt_ob if not done else env.reset()\n",
    "            i+=1\n",
    "            if i == num_samples - 1: break\n",
    "        \n",
    "        # get logprobs of taking actions w.r.t current policy\n",
    "        obs, actions = replay_buffer.get_actions()\n",
    "        logprobs = sess.run(policy.logprob, feed_dict={\n",
    "            policy.obs: obs,\n",
    "            policy.act: actions\n",
    "        })\n",
    "        replay_buffer.set_logprobs(logprobs)\n",
    "        replay_buffer.flush_temp()\n",
    "        \n",
    "    def choose_action(self, obs):\n",
    "        # Greedy action for now\n",
    "        return self.policy.get_best_action(obs)\n",
    "            \n",
    "    def train(self, batch_size):\n",
    "        obs, acts, rewards, nxt_obs, dones, logprobs = self.replay_buffer.get_batch(batch_size)\n",
    "        # inject exploration bonus\n",
    "        rewards = self.density.modify_reward(obs, rewards)\n",
    "\n",
    "        # train actor\n",
    "        adv = policy.estimate_adv(obs, rewards, nxt_obs, dones)\n",
    "        self.policy.train_actor(obs, acts, logprobs, adv)\n",
    "        \n",
    "        # train critic\n",
    "        self.policy.train_critic(obs, nxt_obs, rewards, dones)\n",
    "        \n",
    "        # train density model\n",
    "        s1, s2, target = self.replay_buffer.get_density_batch(obs, batch_size)\n",
    "        self.density.update(s1, s2, target)\n",
    "    \n",
    "    def test(self, num_tests, render=False, max_steps=5000):\n",
    "        obs = self.env.reset()\n",
    "        frames = [self.env.render(mode='rgb_array')]\n",
    "        i, step = 0, 0\n",
    "        try:\n",
    "            while i < num_tests:\n",
    "                if render:\n",
    "                    frames.append(env.render(mode='rgb_array'))\n",
    "                act = self.policy.get_best_action(obs)\n",
    "                nxt_ob, rew, done, _ = env.step(act)\n",
    "                replay_buffer.record(obs, act, rew, nxt_ob, done)\n",
    "                obs = nxt_ob\n",
    "                if done or step > max_steps:\n",
    "                    obs = env.reset()\n",
    "                    i += 1\n",
    "                    step = 0\n",
    "                step += 1\n",
    "        finally:\n",
    "            self.env.close()\n",
    "            \n",
    "        totalr, stdr = replay_buffer.get_temp_reward_info()\n",
    "        logger.log(totalr / i, stdr)\n",
    "        logger.log_frames(frames)\n",
    "        frames = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brennan/anaconda2/envs/py3/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "# test density fcn \n",
    "env = gym.make('MontezumaRevenge-v0')\n",
    "# default params taken from homework\n",
    "d_graph_args = {\n",
    "    'ob_dim': env.observation_space.shape,\n",
    "    'learning_rate': 5e-3,\n",
    "    'z_size': 32,\n",
    "    'kl_weight': 1e-2,\n",
    "    'conv_depth': 5,\n",
    "    'hid_size': 32,\n",
    "    'n_hidden': 4\n",
    "}\n",
    "p_graph_args = {\n",
    "    'ob_dim': env.observation_space.shape,\n",
    "    'act_dim': env.action_space.n,\n",
    "    'clip_range': 0.2,\n",
    "    'conv_depth': 5,\n",
    "    'filter_size': 32,\n",
    "    'learning_rate': 5e-3,\n",
    "    'num_target_updates': 10,\n",
    "    'num_grad_steps_per_target_update': 10\n",
    "}\n",
    "adv_args = {\n",
    "    'gamma': 0.9999999\n",
    "}\n",
    "\n",
    "# build graph\n",
    "# models\n",
    "policy = Policy(p_graph_args, adv_args)\n",
    "density_model = DensityModel(d_graph_args)\n",
    "# utils\n",
    "replay_buffer = MasterBuffer(max_size=50000)\n",
    "logger = Logger()\n",
    "\n",
    "# agent which will do all the work\n",
    "agent = Agent(env, policy, density_model, replay_buffer, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to train...\n",
      "completed itr 0...\n",
      "completed itr 1...\n",
      "completed itr 2...\n",
      "completed itr 3...\n",
      "completed itr 4...\n",
      "completed itr 5...\n",
      "completed itr 6...\n",
      "completed itr 7...\n",
      "completed itr 8...\n",
      "completed itr 9...\n",
      "completed itr 10...\n",
      "completed itr 11...\n",
      "completed itr 12...\n",
      "completed itr 13...\n",
      "completed itr 14...\n",
      "completed itr 15...\n",
      "completed itr 16...\n",
      "completed itr 17...\n",
      "completed itr 18...\n",
      "completed itr 19...\n",
      "completed itr 20...\n",
      "completed itr 21...\n",
      "completed itr 22...\n",
      "completed itr 23...\n",
      "completed itr 24...\n",
      "completed itr 25...\n",
      "completed itr 26...\n",
      "completed itr 27...\n",
      "completed itr 28...\n",
      "completed itr 29...\n",
      "completed itr 30...\n",
      "completed itr 31...\n",
      "completed itr 32...\n",
      "completed itr 33...\n",
      "completed itr 34...\n",
      "completed itr 35...\n",
      "completed itr 36...\n",
      "completed itr 37...\n",
      "completed itr 38...\n",
      "completed itr 39...\n",
      "completed itr 40...\n",
      "completed itr 41...\n",
      "completed itr 42...\n",
      "completed itr 43...\n",
      "completed itr 44...\n",
      "completed itr 45...\n",
      "completed itr 46...\n",
      "completed itr 47...\n",
      "completed itr 48...\n",
      "completed itr 49...\n",
      "completed itr 50...\n",
      "render complete...\n"
     ]
    }
   ],
   "source": [
    "tf_config = tf.ConfigProto(inter_op_parallelism_threads=1, intra_op_parallelism_threads=1)\n",
    "# tf_config.gpu_options.allow_growth = True\n",
    "with tf.Session(config=tf_config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    agent.set_session(sess)\n",
    "    \n",
    "    n_iter = 101\n",
    "    num_samples = 200\n",
    "    batch_sizes = [64]\n",
    "    render_n = 50\n",
    "    \n",
    "    print('starting to train...')\n",
    "    for bs in batch_sizes:\n",
    "        logger.set_tag('bs: '+str(bs))\n",
    "        replay_buffer.flush()\n",
    "        \n",
    "        for itr in range(n_iter):\n",
    "            agent.sample_env(num_samples)\n",
    "            agent.train(batch_size=bs)\n",
    "            print('completed itr {}...\\r'.format(str(itr)))\n",
    "            if itr % render_n == 0 and itr != 0:\n",
    "                agent.test(1, render=True)\n",
    "                print('render complete...')\n",
    "        logger.package_results()\n",
    "        logger.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAEWCAYAAABBvWFzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xm4XePd//H3R4IgkRBDQkRClMRYPaZGWzVTRHEZSqWGav2qSltt+mjNfS406CDqyVNTKVFKG0+r5rE15ATVJoIjiINEZEBEEP3+/lj3iZXTc/bZyVn7bPvsz+u69nXWute91/quvZL93fd9r0ERgZmZWWetUO0AzMyse3BCMTOzQjihmJlZIZxQzMysEE4oZmZWCCcUMzMrhBOK1RxJvSSFpEHVjmVZSPqmpLurHceykPSSpO2qHcfykDRc0pvVjqOeOKHUKUn3S5onaeWC1rcg9/q3pPdy80d28N69JTV1YtsTJL2ftjVX0l8lDVve9dUqSS/kPvOPJC3KzX93edYZEUMiYtJyxNI7Jf0BaX6spMuXJ4Zl2OabknZsmY+IZyJirUpu05bmhFKHJA0BPgcEcEAR64yI3i0vYAawf67sd0VsowPnpm0PAuYB/9MF22yXpB5dvc2I2Dh3DCYBx+eOwcVtxNizq2NcXrUUaz1zQqlPRwOPAlcDo1sKJe0gaWb+y1DSlyU9naZXkXRNatk8I+kHkprL2WB67zhJr0tqlvQzSStK6g/cCmyU+zXdX9JISY9Jmi/pNUmXlPOlEhELgZuAbVpt/xuSnk0tmD9LWj+VXyDpZ7kY35d0bppfPf3K7y2pp6Q/SJqVYrpP0qa59U+Q9EtJd0p6F9hJ0jqS/iLpbUmPABuW+Hzuk3R8q7JpkvaV1CN9drMlvSXpH/ltl0vSSZLuknS5pHnA9yWNkPRg+lzekHSVpN659yz51Z9aGb+VdKOkd1IcW5ax3YOBk4Hj0vH9eyrvL+m69G9uhqQfS9LyxCrpVqA/cG/axv+TtIWkRbk4NpR0e/r3+6yko3LLlmvfbGlOKPXpaOB36bWXpHUBIuIx4F1g11zdrwDXp+kzgSHARsAewFGU72xgK2BL4DPALsAPImIO8GVgeu7X9BzgQ+Aksi+JzwH7A8e3teI8SX2Aw4GmXNlhwClpHesCTwLXpcUPpFgAdgKagc+n+Z2Bf0TEgjT/J2BjYAAwDbim1eaPAn4C9CFrIYwH5qZtnggcWyL0G4AjcjF/hmzf7wT2A7ZN216D7JjMK7GuUr4IPAasBfwilZ2RYtwa2BwYU+L9B5O1/voBDwKXdLTBiPgD8EvginR8P5sWTQBmA0OBHYFDyfZtmWONiC8Dc4Bd0zYuy8eQEtUfgClkx++rwK8k7dCZfbNWIsKvOnqRfUl+CKyV5qcBp+aWnwdcmab7kCWYDdP8dGCvXN3jgeY2tvESsHurslfJ/rO3zI8CpqXpvYGmDuIeA9yQpnuRddcNSvMTgPeA+am8CRiRe+99wJG5+RXTZ7AusDrwftrXs4DvA6+nbVwAXNhOPAOAfwO9cjGMzy3vlZYPyZVdDNzdzvrWTPswMM1fBFyWpvcl+yLcHlihzOP8KHBUq7KTgKkdvO8o4KHc/JvAjml6LPDH3LLtgTfbWU/vdCwG5N57eW75xsDbQM9c2deB24qINc1vASxK08OBhS3HK5X9Crh0WffNr/ZfbqHUn9HAnRHRcvbL9eS6vdL8QcoG6w8CnoiIl9Oy9YBXcnXz0+1Kvw4HAC/nil8G1i/xnhGpe2KWpLfJfpmWGmD9aUT0I2s9fQTkB+U3BC5PXVXzyX4VLyZLSG8D/yRrBX2eLPlMAnYAvkDWgiF1eV0kaXqKZxogslZEi/znMSAtz5fl938pETEXuAs4VNIKwGFkLUiA24EryH49z5R0Wb5bahktdcwkDZJ0c+pWfBu4nNKf88zc9EKyxLE8NgRWA97MHZeLyJJ8UbHmrQfMiohFubLW/waL2re65YRSRyStQtat8IXUbz0TOBXYWtLWABExlew/2j4s3d0F2S/3/Km6G5Sz3ch+8s1k6TGEwWStFsh+ybb2v8ATwMYRsTpwDtkXdEfbepGslfErSSul4leAr0VEv9xrlYiYnJY/QNaFNxx4Ks1/iWwc5uFU55hU54tAX2CzVJ6PKb8fM9N8/jMa3EH4Ld1eXyBrQf097VNExMUR8WmybsOtge90sK72tP6sLwLeImvRrQ58kzI+5wK2+0ra7hq5Y7J6RGzfiVhL3Tr9NWBdLX1WY/7foBXACaW+HEj2630E2ZflNmRfog+Rjau0uJ7sC+vzZAPcLX4P/EjSGmlQ+6Rl2PYNwJlpIHYd4HQ+HseYBazT6ld3H+CtiFggaXOy7pCyRMRtZN0px6Siy4Eftwxkp/gPzr3lAeA4stbYR8D9ZF9WUyLirVw8i8j66Vcj6xosFcMi4DbgbGWD/VsBJU+fJhuj2Rz4L2BCSsRI2lFSg7KTEt4FPiDrTitCH+Ad4G1lZ/+dWtB6W5sFDG0ZdI+I58nGsv5b2UkPK0j6lKSRnYh1FlkLtS3T0utcSStJaiDrMuuKMxDrhhNKfRkNXBURMyJiZssLuBQ4Uh+fRXUD2a/ke3NdY5C1EpqBF4G7gZvJxh/KcQYwlWws4Cngb8CFadk/gInAy6n7Y02yL4vjJS0AxgE3LuO+jgXGSFoxIm5I+3hL6ip5iqy10eIhsiTxYJp/iuwL+8FcnSvIuspmknWRPUzHvkHWhTOLrLvqqlKVIztDbSKwO0u3DPuRnZE3n2wc62U+HqTurB+TtbreJjueN5WuvtyuJ9uPuZIeSmWHkn0+z5KdvHADsHYnYj0PuDCdxXVifkFKzgeTte5mpXhOjYhHOrNTtjSlH0Fmyyz9pz08Ir5Q7VjMrPrcQrGySRqo7PqQFVL30ffIriExM8NXn9qyWIms62YoWffLBOCyku8ws7rhLi8zMyuEu7zMzKwQddXltdZaa8WQIUOqHYaZWU2ZPHnymxFR6gw8oM4SypAhQ2hsbKx2GGZmNUVSu3d5yHOXl5mZFcIJxczMCuGEYmZmhairMZS2fPjhhzQ3N7No0aKOK3dTvXr1YtCgQay44orVDsXMaljdJ5Tm5mb69OnDkCFDSPetqysRwZw5c2hubmbo0KHVDsfMaljdd3ktWrSI/v3712UyAZBE//7967qFZmbFqPuEAtRtMmlR7/tvZsVwQjEzs0I4oVTZSy+9xBZbbFHIumbMmMGee+7J8OHDGTFiBC+99NJSy08++WR69/ZTTc2sMup+UL47Ofroozn99NPZY489WLBgASus8PHvhcbGRubNm1fF6Mysu3ML5RNg8eLFHHnkkQwfPpxDDjmEhQsXAjBmzBhGjBjBVlttxfe///2S65g6dSqLFy9mjz2yBxH27t2bVVddFYCPPvqI0047jQsvvLDUKszMOsUtlJyzb5vC1NfeLnSdI9ZbnTP337xknWeffZYrrriCkSNHcuyxx3LZZZdxzDHHcOuttzJt2jQkMX/+fAAmTpxIY2Mj55xzzlLreO655+jXrx8HHXQQL774Irvvvjvnn38+PXr04NJLL+WAAw5g4MCBhe6bmVmeWyifABtssAEjR44E4KijjuLhhx+mb9++9OrVi+OOO45bbrllSWvjgAMO+I9kAlkr56GHHmLs2LFMmjSJ6dOnc/XVV/Paa69x00038e1vf7tL98nM6o9bKDkdtSQqpfVpu5Lo2bMnjz/+OPfccw8333wzl156Kffee2+76xg0aBDbbLMNG220EQAHHnggjz76KAMGDKCpqYlhw4YBsHDhQoYNG0ZTU1PldsjM6pITyifAjBkzeOSRR9hpp524/vrr2XnnnVmwYAELFy5k3333ZeTIkUsSRXu222475s+fz+zZs1l77bW59957aWho4Etf+hIzZ85cUq93795OJmZWEe7y+gTYdNNNGTduHMOHD2fevHmceOKJvPPOO+y3335stdVW7Lzzzlx88cVANoZyxhln/Mc6evTowdixY9ltt93YcsstiQi+/vWvd/WumFkdq6tnyjc0NETrB2w988wzDB8+vEoRfXL4czCz9kiaHBENHdVzC8XMzArhhGJmZoVwQiG7hXs9q/f9N7Ni1H1C6dWrF3PmzKnbL9WW56H06tWr2qGYWY2r+9OGBw0aRHNzM7Nnz652KFXT8sRGM7POqPuEsuKKK/pJhWZmBaj7Li8zMyuGE4qZmRWiqglF0t6SnpXUJGlMG8tXlnRjWv6YpCGtlg+WtEBS6Xu7m5lZxVUtoUjqAYwD9gFGAEdIGtGq2nHAvIgYBlwCXNBq+cXA7ZWO1czMOlbNFsr2QFNETI+ID4AJwKhWdUYB16Tpm4HdlG7NK+lA4EVgShfFa2ZmJVQzoawPvJKbb05lbdaJiMXAW0B/Sb2BHwJnd7QRSSdIapTUWM+nBpuZVVqtDsqfBVwSEQs6qhgR4yOiISIa1l577cpHZmZWp6p5HcqrwAa5+UGprK06zZJ6An2BOcAOwCGSLgT6Af+WtCgiLq182GZm1pZqJpRJwCaShpIljsOBr7SqMxEYDTwCHALcG9k9Uj7XUkHSWcACJxMzs+qqWkKJiMWSTgLuAHoAV0bEFEnnAI0RMRG4ArhWUhMwlyzpmJnZJ1DdP2DLzMxK8wO2zMysSzmhmJlZIZxQzMysEE4oZmZWCCcUMzMrhBOKmZkVwgnFzMwK4YRiZmaFcEIxM7NCOKGYmVkhnFDMzKwQTihmZlYIJxQzMyuEE4qZmRXCCcXMzArhhGJmZoVwQjEzs0I4oZiZWSGcUMzMrBBOKGZmVggnFDMzK4QTipmZFcIJxczMCuGEYmZmhXBCMTOzQjihmJlZIZxQzMysEE4oZmZWCCcUMzMrhBOKmZkVoqoJRdLekp6V1CRpTBvLV5Z0Y1r+mKQhqXwPSZMl/TP93bWrYzczs6VVLaFI6gGMA/YBRgBHSBrRqtpxwLyIGAZcAlyQyt8E9o+ILYHRwLVdE7WZmbWnmi2U7YGmiJgeER8AE4BRreqMAq5J0zcDu0lSRDwZEa+l8inAKpJW7pKozcysTdVMKOsDr+Tmm1NZm3UiYjHwFtC/VZ2DgSci4v0KxWlmZmXoWe0AOkPS5mTdYHuWqHMCcALA4MGDuygyM7P6U80WyqvABrn5QamszTqSegJ9gTlpfhBwK3B0RLzQ3kYiYnxENEREw9prr11g+GZmltduC0XSk0C0tzwitu3kticBm0gaSpY4Dge+0qrORLJB90eAQ4B7IyIk9QP+DIyJiL91Mg4zMytAqS6vQ9LfbwI9+PhMqiOBjzq74YhYLOkk4I60/isjYoqkc4DGiJgIXAFcK6kJmEuWdABOAoYBZ0g6I5XtGRFvdDYuMzNbPopotxGSVZCeaN0aaausFjQ0NERjY2O1wzAzqymSJkdEQ0f1yhlD6SFpx9yKdyBrUZiZmS1RzllexwFXS+qV5t8Djq1cSGZmVotKJpR0NfuGEbGFpP4AETGnSyIzM7OaUrLLKyI+Av4rTc9xMjEzs/aUM4Zyp6RTJA2UtHrLq+KRmZlZTSlnDOWo9Pd7ubIAfNm5mZkt0WFCiYgNOqpjZmZW1r28JG1Gdov5ljO9iIjrKxWUmZnVng4TiqQfk918cTOyq9r3Ah4GnFDMzGyJcgblDwO+CLweEV8FtgZWq2hUZmZWc8pJKO+l04cXS+oDzAQ2rGxYZmZWa8oZQ3ky3d33SqAReBt4vKJRmZlZzSnnLK9vpMlxku4AVo+IJyoblpmZ1ZpyBuWvAh4EHoqIpsqHZGZmtaicMZTrgaHA/0p6QdKNkr5V4bjMzKzGlNPldZeku4HPALsB30rT4yocm5mZ1ZByurzuIHuW+yTgIWDHiHit0oGZmVltKafL6zlgMbAJ8ClgmKSVKhqVmZnVnHK6vL4NIKkvcDTZs+XXAVapbGhmZlZLyuny+ibwOWA74DXgt2RdX2ZmZkuUc2FjP+AyYFJEfFDheMzMrEZ1OIYSEecDHwGHA0haU5KfhWJmZksp927DI4GNybq7ViG7NmXnyoZmZma1pJyzvA4B9gXeBYiIVwE/AtjMzJZSTkJ5PyKC7LG/SFq1siGZmVktKieh3CJpHNBX0jHAncBVlQ3LzMxqTTnXoVwgaR/gA7KHa/00Im6veGRmZlZTynqmfEogtwMoc1hE3FjRyMzMrKa02+Ulqbek0yT9XNKuKZF8E3iB7Ip5MzOzJUq1UK4DFgCPkN1h+HRgZeDQiGjsgtjMzKyGlEooG0fElgCSLid7lvzgiHivSyIzM7OaUuosrw9bJiLiI+CVopOJpL0lPSupSdKYNpavnB7o1STpMUlDcst+lMqflbRXkXGZmdmyK9VC2VrS3DQtoE+aFxARsWZnNiypB9lDuvYAmoFJkiZGxNRcteOAeRExTNLhwAXAYZJGkN0KZnNgPeBuSZ9Kic/MzKqgVEKp9DNPtgeaImI6gKQJwCggn1BGAWel6ZuBSyUplU+IiPeBFyU1pfU9UolAz75tClNfe7sSqzYzq7gR663OmftvXvHtlOryWq2DV2etD7ySm29OZW3WiYjFwFtA/zLfC4CkEyQ1SmqcPXt2AWGbmVlbSrVQppDdbkVtLAugJu44HBHjgfEADQ0NsTzr6IrMbmZW69pNKBGxQYW3/SqQ38agVNZWnWZJPcmebT+nzPeamVkXKudeXkjqK2lbSZ9teRWw7UnAJpKGpmfUHw5MbFVnIjA6TR8C3JtuVDkRODydBTaU7Hn3jxcQk5mZLadynodyHPBdsjGKf5I9CvhRYJfObDgiFks6CbgD6AFcGRFTJJ0DNEbEROAK4No06D6X9JCvVO/3ZAP4i4Fv+QwvM7PqUvaDv0QF6Z+kM6giYhtJmwPnRMTBXRFgkRoaGqKx0Rf5m5ktC0mTI6Kho3rldHktarmgUdJKETEF2LSzAZqZWfdSzt2GX5fUD7gNuCNd3Nhc2bDMzKzWlPM8lAPS5E8k7UZ2ptX/VTQqMzOrOR12eUm6umU6Iu6JiFtI13WYmZm1KGcMZav8jKQVyM70MjMzW6LUA7Z+KGkesJWkuZLmpfk3gb90WYRmZlYTSrVQLgTWBi5Jf9cC1oqINSPitK4IzszMakepW68E2UWDp0naF/g8gKT7I+KvXRSfmZnViHIG5c8DfgBMT68fpDIzM7MlyrkO5QDg0y23NpF0JfAE8ONKBmZmZrWlrJtDAqvnpvtUIhAzM6tt5bRQLgSekHQP2bNRdgF+UsmgzMys9rSbUCQNjogZEXGdpPuAHdKiMyLCzx4xM7OllGqh/BHYFiAlkFu6JCIzM6tJpcZQ2nr0r5mZWZtKtVDWl/TL9hZGxMkViMfMzGpUqYTyHjC5qwIxM7PaViqhzImIa7osEjMzq2mlxlA+6LIozMys5pW6l9eOAJK2bWPxW8DLEbG4UoGZmVltKefCxsvITh9+muzMry2AKUBfSSdGxJ0VjM/MzGpEObdeeY3sXl4NEfEZ4NNkN4ncg+wqejMzs7ISyqciYkrLTERMBTaLiOmVC8vMzGpNOV1eUyT9GpiQ5g8DpkpaGfiwYpGZmVlNKaeF8jWgCTglvaansg+BL1YqMDMzqy3ltFD2AS6NiIvaWLag4HjMzKxGldNC2R94TtK1kvaTVE4SMjOzOtNhQomIY4BhwE3AEcALkn5T6cDMzKy2lNXaiIgPJd0OBLAq8GXg+EoGZmZmtaXDFoqkfSRdDTwPHAyMB9atcFxmZlZjyhlDORq4Fdg0Ir5GNhD/i85sVNKaku6S9Hz6u0Y79UanOs9LGp3KVpX0Z0nTJE2RdH5nYjEzs2KUM4ZyBDADOFfSS8A5wLRObncMcE9EbALck+aXImlN4EyyRw9vD5yZSzxjI2Izsqv2R0rap5PxmJlZJ5V6pvynyAbhjwDeBG4EFBFFXHsyCtglTV8D3A/8sFWdvYC7ImJuiucuYO+IuAG4DyAiPpD0BDCogJjMzKwTSrVQpgG7AvtFxM4R8Svgo4K2u25EvJ6mZ9L2mMz6wCu5+eZUtoSkfmSnNd9TUFxmZracSp3ldRBwOHCfpL+S3Xql7OfMS7obGNDGotPzMxERkqLc9ebW3xO4AfhlqfuKSToBOAFg8ODBy7oZMzMrU6nnofwR+KOk1ci6qE4B1kn39bq1o9vWR8Tu7S2TNEvSwIh4XdJA4I02qr3Kx91ikHVr3Z+bHw88HxE/7yCO8akuDQ0Ny5y4zMysPOUMyr8bEddHxP5kX+pP8p/jHctqIjA6TY8G/tRGnTuAPSWtkQbj90xlSDoP6EuW5MzM7BOgnNOGl4iIeRExPiJ26+R2zwf2kPQ8sHuaR1JDy1X4aTD+XGBSep0TEXMlDSLrNhsBPCHpKUm+yNLMrMoUUT+9QA0NDdHY2FjtMMzMaoqkyRHR0FG9ZWqhmJmZtccJxczMCuGEYmZmhXBCMTOzQjihmJlZIZxQzMysEE4oZmZWCCcUMzMrhBOKmZkVwgnFzMwK4YRiZmaFcEIxM7NCOKGYmVkhnFDMzKwQTihmZlYIJxQzMyuEE4qZmRXCCcXMzArhhGJmZoVwQjEzs0I4oZiZWSGcUMzMrBBOKGZmVggnFDMzK4QTipmZFcIJxczMCuGEYmZmhXBCMTOzQjihmJlZIZxQzMysEE4oZmZWiKokFElrSrpL0vPp7xrt1Bud6jwvaXQbyydK+lflIzYzs45Uq4UyBrgnIjYB7knzS5G0JnAmsAOwPXBmPvFIOghY0DXhmplZR6qVUEYB16Tpa4AD26izF3BXRMyNiHnAXcDeAJJ6A98FzuuCWM3MrAzVSijrRsTraXomsG4bddYHXsnNN6cygHOBi4CFHW1I0gmSGiU1zp49uxMhm5lZKT0rtWJJdwMD2lh0en4mIkJSLMN6twE2johTJQ3pqH5EjAfGAzQ0NJS9HTMzWzYVSygRsXt7yyTNkjQwIl6XNBB4o41qrwK75OYHAfcDOwENkl4ii38dSfdHxC6YmVnVVKvLayLQctbWaOBPbdS5A9hT0hppMH5P4I6I+HVErBcRQ4CdgeecTMzMqq9aCeV8YA9JzwO7p3kkNUj6DUBEzCUbK5mUXuekMjMz+wRSRP0MKzQ0NERjY2O1wzAzqymSJkdEQ0f1fKW8mZkVwgnFzMwK4YRiZmaFcEIxM7NCOKGYmVkhnFDMzKwQTihmZlYIJxQzMyuEE4qZmRXCCcXMzArhhGJmZoVwQjEzs0I4oZiZWSGcUMzMrBBOKGZmVggnFDMzK4QTipmZFcIJxczMCuGEYmZmhXBCMTOzQjihmJlZIZxQzMysEE4oZmZWCCcUMzMrhCKi2jF0GUmzgZeX8+1rAW8WGE4tqMd9hvrc73rcZ6jP/V6efd4wItbuqFJdJZTOkNQYEQ3VjqMr1eM+Q33udz3uM9Tnfldyn93lZWZmhXBCMTOzQjihlG98tQOognrcZ6jP/a7HfYb63O+K7bPHUMzMrBBuoZiZWSGcUMzMrBBOKB2QtLekZyU1SRpT7XgqRdIGku6TNFXSFEnfSeVrSrpL0vPp7xrVjrVoknpIelLS/6X5oZIeS8f8RkkrVTvGoknqJ+lmSdMkPSNpp+5+rCWdmv5t/0vSDZJ6dcdjLelKSW9I+leurM1jq8wv0/4/LWnbzmzbCaUEST2AccA+wAjgCEkjqhtVxSwGvhcRI4AdgW+lfR0D3BMRmwD3pPnu5jvAM7n5C4BLImIYMA84ripRVdYvgL9GxGbA1mT7322PtaT1gZOBhojYAugBHE73PNZXA3u3Kmvv2O4DbJJeJwC/7syGnVBK2x5oiojpEfEBMAEYVeWYKiIiXo+IJ9L0O2RfMOuT7e81qdo1wIHVibAyJA0CvgT8Js0L2BW4OVXpjvvcF/g8cAVARHwQEfPp5sca6AmsIqknsCrwOt3wWEfEg8DcVsXtHdtRwG8j8yjQT9LA5d22E0pp6wOv5OabU1m3JmkI8GngMWDdiHg9LZoJrFulsCrl58APgH+n+f7A/IhYnOa74zEfCswGrkpdfb+RtBrd+FhHxKvAWGAGWSJ5C5hM9z/WLdo7toV+xzmh2FIk9Qb+AJwSEW/nl0V2jnm3Oc9c0n7AGxExudqxdLGewLbAryPi08C7tOre6obHeg2yX+NDgfWA1fjPbqG6UMlj64RS2qvABrn5QamsW5K0Ilky+V1E3JKKZ7U0gdPfN6oVXwWMBA6Q9BJZd+auZGML/VK3CHTPY94MNEfEY2n+ZrIE052P9e7AixExOyI+BG4hO/7d/Vi3aO/YFvod54RS2iRgk3QmyEpkg3gTqxxTRaSxgyuAZyLi4tyiicDoND0a+FNXx1YpEfGjiBgUEUPIju29EXEkcB9wSKrWrfYZICJmAq9I2jQV7QZMpRsfa7Kurh0lrZr+rbfsc7c+1jntHduJwNHpbK8dgbdyXWPLzFfKd0DSvmT97D2AKyPip1UOqSIk7Qw8BPyTj8cT/otsHOX3wGCyW/8fGhGtB/xqnqRdgO9HxH6SNiJrsawJPAkcFRHvVzO+oknahuxEhJWA6cAxZD8wu+2xlnQ2cBjZGY1PAseTjRd0q2Mt6QZgF7Lb1M8CzgT+SBvHNiXXS8m6/xYCx0RE43Jv2wnFzMyK4C4vMzMrhBOKmZkVwgnFzMwK4YRiZmaFcEIxM7NCOKFYXZPUX9JT6TVT0qu5+bLuPCvpqtw1He3VOU/SKWn6WEkDiog/rW/XdA1By/y3JB1Z1PrNytWz4ypm3VdEzAG2AZB0FrAgIsbm66Rz9RUR//7PNUBEHLOMmz0WeILsnkplkdQzd8+p1nYF3gQeTfGMW8Z4zArhFopZGyQNS8+G+R0wBRgoabykxvRMjTNydR+WtI2knpLmSzpf0j8kPSJpnVbrPYwsgd3Y0gqStJ2kByRNlnS7pHVz671EUiNwkqRR6dkdT0q6U9I6kjYmu0DvtLS+z7ZqDW2b3vO0pD+kOw23rPt8SY8re97PZ7vkg7VuzQnFrH2bkT0rY0S6W+2YiGgge37IHu08G6cv8EBEbA08QtYaWSIibgSeAg6LiG0Akd0/7OCI+AxwHXBu7i09IqIZwkrSAAABiElEQVQhIn4OPAjsmG7oeAvZ82teILvi/WcRsU1E/L1VPNcB342IrYBngZ/klikitgdOA87ArJPc5WXWvhda3YbiCEnHkf2/WY/soWtTW73nvYi4PU1PBj7XwTaGA5sDd2c9a/Qgu3ljixtz04OB36fxl5WB50qtWFJ/oFdE/C0VXQNcm6vScgPQycCQDuI065ATiln73m2ZkLQJ2ZMdt4+I+ZKuA3q18Z4PctMf0fH/MQFPR0R7iefd3PQ44L8j4i+SdqfzT1RsuWdVOXGadchdXmblWR14B3g73f57r06s6x2gT5qeCqwvaXuANKayeTvv6wu8mk4SGJ0rz69viXTCwXu58ZGvAg90Im6zkpxQzMrzBNmX/zTgt8DfSlcv6SrgN5KeInvQ0SHAxZKeJrvj7Q7tvO8s4FayxyrMypX/CTg0Dda3Hlz/KnBJWvcI4LxOxG1Wku82bGZmhXALxczMCuGEYmZmhXBCMTOzQjihmJlZIZxQzMysEE4oZmZWCCcUMzMrxP8HJCGX7o2y7F0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for results in logger.results:\n",
    "    totalr, stdr = results[0], results[1]\n",
    "    plt.plot(totalr)\n",
    "plt.title('AvgTotalReward vs Train Iteration')\n",
    "plt.xlabel('TrainIteration')\n",
    "plt.ylabel('AvgTotalReward')\n",
    "plt.legend(logger.tags, loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- transport code to GPU environment to test density function\n",
    "- split density training into seperate function <b>later</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('iter-frames/nsamples: 100-0'),\n",
       " PosixPath('iter-frames/WithoutNormBonus'),\n",
       " PosixPath('iter-frames/nsamples: 100-1')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs = list(Path('iter-frames').iterdir())\n",
    "logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_frames = 50\n",
    "for fn in reversed(logs):\n",
    "    if os.path.isfile(fn):\n",
    "        with open(fn, 'rb') as file:\n",
    "            frames = pickle.load(file)\n",
    "    #         frames, rewards = data[0], data[1]\n",
    "            # View \n",
    "            img = plt.imshow(frames[0])\n",
    "            plt.title('Iteration:' + str(fn))\n",
    "            for i, frame in enumerate(frames):\n",
    "                img.set_data(frame) # just update the data\n",
    "                display.display(plt.gcf())\n",
    "                display.clear_output(wait=True)\n",
    "                if i > n_frames: break          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
