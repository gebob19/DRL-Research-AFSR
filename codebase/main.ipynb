{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np \n",
    "import copy\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import gym \n",
    "\n",
    "# rendering gym env\n",
    "from IPython import display\n",
    "import matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/gym/envs/registration.py:14: DeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MontezumaRevenge-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Discrete(18), Box(210, 160, 3))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space, env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Network(input_tensor, output_size, scope, fsize, conv_depth, n_hidden_dense=0, activation=tf.tanh, output_activation=None):\n",
    "        with tf.variable_scope(scope):\n",
    "            x = input_tensor\n",
    "            # Convolutions\n",
    "            for _ in range(conv_depth):\n",
    "                x = tf.layers.conv2d(x, fsize, (3, 3), activation='relu')\n",
    "                x = tf.layers.conv2d(x, fsize, (3, 3), strides=(2, 2))\n",
    "            \n",
    "            # Dense Layers\n",
    "            x = tf.layers.flatten(x)\n",
    "            for _ in range(n_hidden_dense):\n",
    "                x = tf.layers.dense(x, fsize, activation=activation)\n",
    "            y = tf.layers.dense(x, output_size, activation=output_activation)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(object):\n",
    "    def __init__(self, graph_args, adv_args):\n",
    "        self.ob_dim = graph_args['ob_dim']\n",
    "        self.act_dim = graph_args['act_dim']\n",
    "        clip_range = graph_args['clip_range']\n",
    "        # conv operations params\n",
    "        conv_depth = graph_args['conv_depth']\n",
    "        filter_size = graph_args['filter_size']\n",
    "        \n",
    "        self.learning_rate = graph_args['learning_rate']\n",
    "        self.num_target_updates = graph_args['num_target_updates']\n",
    "        self.num_grad_steps_per_target_update = graph_args['num_grad_steps_per_target_update']\n",
    "        \n",
    "        self.gamma = adv_args['gamma']\n",
    "        \n",
    "        self.obs, self.act, self.adv, self.old_logprob = self.define_placeholders()\n",
    "        \n",
    "        # policy / actor evaluation\n",
    "        self.policy_distrib = Network(self.obs, self.act_dim, 'policy', filter_size, conv_depth)\n",
    "        self.greedy_action = tf.argmax(self.policy_distrib, axis=1)\n",
    "        self.logprob = self.get_logprob(self.policy_distrib, self.act)\n",
    "        \n",
    "        # importance sampling\n",
    "        ratio = tf.exp(self.logprob - self.old_logprob)\n",
    "        clipped_ratio = tf.clip_by_value(ratio, 1.0-clip_range, 1.0+clip_range)        \n",
    "        # include increase entropy term with alpha=0.2\n",
    "        batch_loss = tf.minimum(ratio*self.adv, clipped_ratio*self.adv) - 0.2 * self.logprob\n",
    "        self.actor_loss = -1 * tf.reduce_mean(batch_loss)\n",
    "        self.actor_update_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.actor_loss)\n",
    "        \n",
    "        # critic definition\n",
    "        self.v_pred = tf.squeeze(Network(self.obs, 1, 'critic', filter_size, conv_depth, n_hidden_dense=2))\n",
    "        self.v_target = tf.placeholder(shape=(None,), name='v_target', dtype=tf.float32)\n",
    "        self.critic_loss = tf.losses.mean_squared_error(self.v_target, self.v_pred)\n",
    "        # minimize with respect to correct variables HERE\n",
    "        self.critic_update_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.critic_loss)\n",
    "        \n",
    "    def set_session(self, sess):\n",
    "        self.sess = sess\n",
    "        \n",
    "    def define_placeholders(self):\n",
    "        obs = tf.placeholder(shape=((None,) + (self.ob_dim)), name='obs', dtype=tf.float32)\n",
    "        act = tf.placeholder(shape=(None,), name='act', dtype=tf.int32)\n",
    "        adv = tf.placeholder(shape=(None,), name='adv', dtype=tf.float32)\n",
    "        logprob = tf.placeholder(shape=(None,), name='logprob', dtype=tf.float32)\n",
    "        return obs, act, adv, logprob\n",
    "    \n",
    "    def get_logprob(self, policy_distribution, actions):\n",
    "        action_enc = tf.one_hot(actions, depth=self.act_dim)\n",
    "        logprob = -1 * tf.nn.softmax_cross_entropy_with_logits_v2(logits=policy_distribution, labels=action_enc)\n",
    "        return logprob\n",
    "        \n",
    "    def get_best_action(self, observation):\n",
    "        act = sess.run(self.greedy_action, feed_dict={\n",
    "            self.obs: [observation]\n",
    "        })[0]\n",
    "        return act\n",
    "    \n",
    "    def estimate_adv(self, obs, rew, nxt_obs, dones):\n",
    "        # V(s) & V(s')\n",
    "        v_obs = self.sess.run(self.v_pred, feed_dict={self.obs: obs})\n",
    "        v_nxt_obs = self.sess.run(self.v_pred, feed_dict={self.obs: nxt_obs})\n",
    "        # y = r + gamma * V(s')\n",
    "        y_obs = rew + (1 - dones) * self.gamma * v_nxt_obs\n",
    "        # Adv(s) = y - V(s)\n",
    "        adv = y_obs - v_obs\n",
    "        # Normalize advantages\n",
    "        adv = (adv - np.mean(adv)) / (np.std(adv) + 1e-8)\n",
    "        return adv\n",
    "    \n",
    "    def train_actor(self, obs, act, logprob, adv):\n",
    "        self.sess.run(self.actor_update_op, feed_dict={\n",
    "            self.obs: obs,\n",
    "            self.act: act,\n",
    "            self.adv: adv,\n",
    "            self.old_logprob: logprob\n",
    "        })\n",
    "        \n",
    "    def train_critic(self, obs, nxt_obs, rew, dones):\n",
    "        for i in range(self.num_grad_steps_per_target_update * self.num_target_updates):\n",
    "            if i % self.num_grad_steps_per_target_update == 0:\n",
    "                v_pred = self.sess.run(self.v_pred, feed_dict={self.obs: nxt_obs})\n",
    "                y = rew + self.gamma * v_pred * (1 - dones)\n",
    "                \n",
    "            _, loss = self.sess.run([self.critic_update_op, self.critic_loss],\n",
    "                                    feed_dict={self.obs: obs, self.v_target: y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self):\n",
    "        self.obs = []\n",
    "        self.acts = []\n",
    "        self.rewards = []\n",
    "        self.nxt_obs = []\n",
    "        self.dones = []\n",
    "        self.logprobs = []\n",
    "    \n",
    "    def record(self, obs, act, rew, nxt_ob, done):\n",
    "        self.obs.append(obs)\n",
    "        self.acts.append(act)\n",
    "        self.rewards.append(rew)\n",
    "        self.nxt_obs.append(nxt_ob)\n",
    "        self.dones.append(done)\n",
    "        \n",
    "    def get_actions(self):\n",
    "        return np.asarray(self.obs), np.asarray(self.acts)\n",
    "    \n",
    "    def set_logprobs(self, logprobs):\n",
    "        self.logprobs += list(logprobs)\n",
    "        assert len(self.logprobs) == len(self.obs), 'logprobs MUST == self.obs'\n",
    "        \n",
    "    def merge(self, logprobs, obs, acts, rews, nxt_obs, dones):\n",
    "        self.obs += obs\n",
    "        self.acts += acts\n",
    "        self.rewards += rews\n",
    "        self.nxt_obs += nxt_obs\n",
    "        self.dones += dones\n",
    "        self.logprobs += list(logprobs)\n",
    "    \n",
    "    def export(self):\n",
    "        return self.obs, self.acts, self.rewards, self.nxt_obs, self.dones\n",
    "    \n",
    "    def get_samples(self, indices):\n",
    "        return (\n",
    "            np.asarray(self.obs)[indices],\n",
    "            np.asarray(self.acts)[indices],\n",
    "            np.asarray(self.rewards)[indices],\n",
    "            np.asarray(self.nxt_obs)[indices],\n",
    "            np.asarray(self.dones)[indices],\n",
    "            np.asarray(self.logprobs)[indices]\n",
    "        )\n",
    "    \n",
    "    def flush(self):\n",
    "        self.obs = []\n",
    "        self.acts = []\n",
    "        self.rewards = []\n",
    "        self.nxt_obs = []\n",
    "        self.dones = []\n",
    "        self.logprobs = []\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.obs)\n",
    "        \n",
    "class MasterBuffer(object):\n",
    "    def __init__(self):\n",
    "        self.master_replay = ReplayBuffer()\n",
    "        self.temp_replay = ReplayBuffer()\n",
    "    \n",
    "    def record(self, *args):\n",
    "        self.temp_replay.record(*args)\n",
    "        \n",
    "    def get_actions(self):\n",
    "        return self.temp_replay.get_actions()\n",
    "    \n",
    "    def get_obs(self):\n",
    "        return np.asarray(self.master_replay.obs)\n",
    "    \n",
    "    def set_logprobs(self, logprobs):\n",
    "        temp_data = self.temp_replay.export()\n",
    "        self.master_replay.merge(logprobs, *temp_data)\n",
    "    \n",
    "    def get_batch(self, batch_size):\n",
    "        indices = np.random.randint(0, len(self.master_replay), batch_size)\n",
    "        return self.master_replay.get_samples(indices)\n",
    "    \n",
    "    ## Density Sampling Start ##\n",
    "    # credit to hw5\n",
    "    def get_density_batch(self, states, batch_size):\n",
    "        if len(self.master_replay) >= 2*len(states):\n",
    "            positives, negatives = self.sample_idxs_replay(states, batch_size)\n",
    "        else:\n",
    "            positives, negatives = self.sample_idxs(states, batch_size)\n",
    "        labels = np.concatenate([np.ones((batch_size, 1)), np.zeros((batch_size, 1))], axis=0)\n",
    "        return positives, negatives, labels\n",
    "    \n",
    "    def sample_idxs(self, states, batch_size):\n",
    "        states = copy.deepcopy(states)\n",
    "        data_size = len(states)\n",
    "        pos_idxs = np.random.randint(data_size, size=batch_size)\n",
    "        continue_sampling = True\n",
    "        while continue_sampling:\n",
    "            neg_idxs = np.random.randint(data_size, size=batch_size)\n",
    "            if np.all(pos_idxs != neg_idxs):\n",
    "                continue_sampling = False\n",
    "        positives = np.concatenate([states[pos_idxs], states[pos_idxs]], axis=0)\n",
    "        negatives = np.concatenate([states[pos_idxs], states[neg_idxs]], axis=0)\n",
    "        return positives, negatives\n",
    "\n",
    "    def sample_idxs_replay(self, states, batch_size):\n",
    "        states = copy.deepcopy(states)\n",
    "        data_size = len(states)\n",
    "        pos_idxs = np.random.randint(data_size, size=batch_size)\n",
    "        neg_idxs = np.random.randint(data_size, len(self.master_replay), size=batch_size)\n",
    "        \n",
    "        buffer_states = self.get_obs()\n",
    "        positives = np.concatenate([states[pos_idxs], states[pos_idxs]], axis=0)\n",
    "        negatives = np.concatenate([states[pos_idxs], buffer_states[neg_idxs]], axis=0)\n",
    "        return positives, negatives\n",
    "    ## Density Sampling End ##        \n",
    "    \n",
    "    def get_temp_reward_info(self):\n",
    "        rewards = np.asarray(self.temp_replay.rewards)\n",
    "        return np.sum(rewards), np.std(rewards)\n",
    "        \n",
    "    def flush_temp(self):\n",
    "        self.temp_replay.flush()\n",
    "    \n",
    "    def flush(self):\n",
    "        self.temp_replay.flush()\n",
    "        self.master_replay.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger(object):\n",
    "    def __init__(self):\n",
    "        self.tag = None\n",
    "        self.totalr = []\n",
    "        self.std_reward = []\n",
    "        \n",
    "        self.tags = []\n",
    "        self.results = []\n",
    "        \n",
    "        self.fset = Path('iter-frames')\n",
    "        self.fset.mkdir(exist_ok=True)\n",
    "        self.n_frames_stored = 0\n",
    "        \n",
    "    def set_tag(self, tag):\n",
    "        self.tag = tag\n",
    "        \n",
    "    def log(self, totalr, std):\n",
    "        self.totalr.append(totalr)\n",
    "        self.std_reward.append(std)\n",
    "        \n",
    "    def log_frames(self, frames):\n",
    "        fname = '{}'.format(self.n_frames_stored)\n",
    "        with open(self.fset/fname, 'wb') as f:\n",
    "            pickle.dump(frames, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "        self.n_frames_stored += 1\n",
    "        \n",
    "    def package_results(self):\n",
    "        # store\n",
    "        self.tags.append(self.tag)\n",
    "        self.results.append([self.totalr, self.std_reward])\n",
    "    \n",
    "    def flush(self):\n",
    "        self.totalr = []\n",
    "        self.std_reward = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Actor Critic Test On Cart-Pole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for results in logger.results:\n",
    "#     totalr, stdr = results[0], results[1]\n",
    "#     plt.plot(totalr)\n",
    "# plt.title('AvgTotalReward vs Train Iteration')\n",
    "# plt.xlabel('TrainIteration')\n",
    "# plt.ylabel('AvgTotalReward')\n",
    "# plt.legend(logger.tags, loc='upper left')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Density Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DensityModel(object):\n",
    "    def __init__(self, graph_args):\n",
    "        # unpacking\n",
    "        self.ob_dim = graph_args['ob_dim']\n",
    "        self.learning_rate = graph_args['learning_rate']\n",
    "        self.z_size = graph_args['z_size']\n",
    "        self.kl_weight = graph_args['kl_weight']\n",
    "        # network operations params\n",
    "        self.hid_size = graph_args['hid_size']\n",
    "        self.n_hidden = graph_args['n_hidden']\n",
    "        \n",
    "        self.state1, self.state2 = self.define_placeholders()\n",
    "        # q(z_1 | s_1), q(z_2 | s_2), p(z), p(y | z)\n",
    "        self.encoder1, self.encoder2, self.prior, self.discriminator = self.forward_pass(self.state1, self.state2)\n",
    "        self.discrim_target = tf.placeholder(shape=[None, 1], name=\"discrim_target\", dtype=tf.float32)\n",
    "\n",
    "        self.log_likelihood = tf.squeeze(self.discriminator.log_prob(self.discrim_target), axis=1)\n",
    "        self.likelihood = tf.squeeze(self.discriminator.prob(self.discrim_target), axis=1)\n",
    "        \n",
    "        self.kl = self.encoder1.kl_divergence(self.prior) + self.encoder2.kl_divergence(self.prior)\n",
    "\n",
    "        assert len(self.log_likelihood.shape) == len(self.likelihood.shape) == len(self.kl.shape) == 1\n",
    "        \n",
    "        self.elbo = tf.reduce_mean(self.log_likelihood - self.kl_weight * self.kl)\n",
    "        self.update_op = tf.train.AdamOptimizer(self.learning_rate).minimize(-self.elbo)\n",
    "\n",
    "    def define_placeholders(self):\n",
    "        state1 = tf.placeholder(shape=((None,) + (self.ob_dim)), name=\"s1\", dtype=tf.float32)\n",
    "        state2 = tf.placeholder(shape=((None,) + (self.ob_dim)), name=\"s2\", dtype=tf.float32)\n",
    "        return state1, state2\n",
    "    \n",
    "    def set_session(self, sess):\n",
    "        self.sess = sess\n",
    "\n",
    "    #Network(input_tensor, output_size, scope, fsize, conv_depth, n_hidden_dense=0, activation=tf.tanh, output_activation=None):\n",
    "    def make_encoder(self, state, z_size, scope):\n",
    "        \"\"\" Encodes the given state to z_size => create guass. distribution for q(z | s)\n",
    "        \"\"\"\n",
    "        # conv operations\n",
    "        z_mean = Network(state, z_size, scope, self.hid_size, conv_depth=self.n_hidden)\n",
    "        z_logstd = tf.get_variable(\"logstd\", shape=(z_size,)) \n",
    "        return tfp.distributions.MultivariateNormalDiag(loc=z_mean, scale_diag=tf.exp(z_logstd))\n",
    "\n",
    "    def make_prior(self, z_size):\n",
    "        \"\"\" Create Prior to map states too => p(z), we will use a normal guass distrib\n",
    "        \"\"\"\n",
    "        prior_mean = tf.zeros((z_size,))\n",
    "        prior_logstd = tf.zeros((z_size,))\n",
    "        return tfp.distributions.MultivariateNormalDiag(loc=prior_mean, scale_diag=tf.exp(prior_logstd))\n",
    "\n",
    "    def make_discriminator(self, z, output_size, scope, n_layers, hid_size):\n",
    "        \"\"\" Predict D(z = [z1, z2]) => p(y | z)\n",
    "        \"\"\"\n",
    "        logit = Network(z, output_size, scope, hid_size, conv_depth=0, n_hidden_dense=n_layers)\n",
    "        return tfp.distributions.Bernoulli(logit)\n",
    "\n",
    "    def forward_pass(self, state1, state2):\n",
    "        # Reuse\n",
    "        make_encoder1 = tf.make_template('encoder1', self.make_encoder)\n",
    "        make_encoder2 = tf.make_template('encoder2', self.make_encoder)\n",
    "        make_discriminator = tf.make_template('decoder', self.make_discriminator)\n",
    "\n",
    "        # Encoder\n",
    "        encoder1 = make_encoder1(state1, self.z_size, 'z1')\n",
    "        encoder2 = make_encoder2(state2, self.z_size, 'z2')\n",
    "\n",
    "        # Prior\n",
    "        prior = self.make_prior(self.z_size)\n",
    "\n",
    "        # Sampled Latent (some noise)\n",
    "        self.z1 = encoder1.sample()\n",
    "        z2 = encoder2.sample()\n",
    "        z = tf.concat([self.z1, z2], axis=1)\n",
    "\n",
    "        # Discriminator\n",
    "        discriminator = make_discriminator(z, 1, 'discriminator', self.n_hidden, self.hid_size)\n",
    "        return encoder1, encoder2, prior, discriminator\n",
    "\n",
    "    def update(self, state1, state2, target):\n",
    "        _, ll, kl, elbo = self.sess.run([self.update_op, self.log_likelihood, self.kl, self.elbo], feed_dict={\n",
    "            self.state1: state1,\n",
    "            self.state2: state2,\n",
    "            self.discrim_target: target\n",
    "        })\n",
    "        return ll, kl, elbo\n",
    "    \n",
    "    def get_encoding(self, state):\n",
    "        \"\"\"Assuming only encode a single state at a time\n",
    "        We will call this to use in our state dynamics fcn\n",
    "        \"\"\"\n",
    "        return self.sess.run(self.z1, feed_dict={\n",
    "            self.state1: [state]\n",
    "        })[0]\n",
    "\n",
    "    def get_likelihood(self, state1, state2):\n",
    "        bs, _, _, _ = state1.shape\n",
    "        target = np.zeros((bs, 1))\n",
    "        for i, (s1, s2) in enumerate(zip(state1, state2)):\n",
    "            if s1.all() == s2.all(): target[i] = [1]\n",
    "\n",
    "        likelihood = self.sess.run(self.likelihood, feed_dict={\n",
    "            self.state1: state1,\n",
    "            self.state2: state2,\n",
    "            self.discrim_target: target\n",
    "        })\n",
    "        return likelihood\n",
    "\n",
    "    def get_prob(self, state):\n",
    "        likelihood = self.get_likelihood(state, state)\n",
    "        # avoid divide by 0 and log(0)\n",
    "        likelihood = np.clip(np.squeeze(likelihood), 1e-5, 1-1e-5)\n",
    "        prob = (1 - likelihood) / likelihood\n",
    "        return prob\n",
    "    \n",
    "    def modify_reward(self, state, rewards):\n",
    "        probs = self.get_prob(state)\n",
    "        bonus = -np.log(probs)\n",
    "        return rewards + 1e-3 * bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, env, policy, density, replay_buffer, logger):\n",
    "        self.env = env\n",
    "        self.policy = policy\n",
    "        self.density = density\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.logger = logger\n",
    "        \n",
    "    def set_session(self, sess):\n",
    "        self.sess = sess\n",
    "        self.policy.set_session(sess)\n",
    "        self.density.set_session(sess)\n",
    "        \n",
    "    def sample_env(self, num_samples):\n",
    "        obs = self.env.reset()\n",
    "        i = 0\n",
    "        while True:\n",
    "            act = self.choose_action(obs)\n",
    "            nxt_ob, rew, done, _ = env.step(act)\n",
    "            \n",
    "            replay_buffer.record(obs, act, rew, nxt_ob, done)\n",
    "            obs = nxt_ob if not done else env.reset()\n",
    "            i+=1\n",
    "            if i == num_samples - 1: break\n",
    "        \n",
    "        # get logprobs of taking actions w.r.t current policy\n",
    "        obs, actions = replay_buffer.get_actions()\n",
    "        logprobs = sess.run(policy.logprob, feed_dict={\n",
    "            policy.obs: obs,\n",
    "            policy.act: actions\n",
    "        })\n",
    "        replay_buffer.set_logprobs(logprobs)\n",
    "        replay_buffer.flush_temp()\n",
    "        \n",
    "    def choose_action(self, obs):\n",
    "        # Greedy action for now\n",
    "        return self.policy.get_best_action(obs)\n",
    "            \n",
    "    def train(self, batch_size):\n",
    "        obs, acts, rewards, nxt_obs, dones, logprobs = self.replay_buffer.get_batch(batch_size)\n",
    "        # inject exploration bonus\n",
    "        rewards = self.density.modify_reward(obs, rewards)\n",
    "\n",
    "        # train actor\n",
    "        adv = policy.estimate_adv(obs, rewards, nxt_obs, dones)\n",
    "        self.policy.train_actor(obs, acts, logprobs, adv)\n",
    "        \n",
    "        # train critic\n",
    "        self.policy.train_critic(obs, nxt_obs, rewards, dones)\n",
    "        \n",
    "        # train density model\n",
    "        s1, s2, target = self.replay_buffer.get_density_batch(obs, batch_size)\n",
    "        self.density.update(s1, s2, target)\n",
    "    \n",
    "    def test(self, num_tests, render=False, max_steps=5000):\n",
    "        obs = self.env.reset()\n",
    "        frames = [self.env.render(mode='rgb_array')]\n",
    "        i, step = 0, 0\n",
    "        try:\n",
    "            while i < num_tests:\n",
    "                if render:\n",
    "                    frames.append(env.render(mode='rgb_array'))\n",
    "                act = self.policy.get_best_action(obs)\n",
    "                nxt_ob, rew, done, _ = env.step(act)\n",
    "                replay_buffer.record(obs, act, rew, nxt_ob, done)\n",
    "                obs = nxt_ob\n",
    "                if done or step > max_steps:\n",
    "                    obs = env.reset()\n",
    "                    i += 1\n",
    "                    step = 0\n",
    "                step += 1\n",
    "        finally:\n",
    "            self.env.close()\n",
    "            \n",
    "        totalr, stdr = replay_buffer.get_temp_reward_info()\n",
    "        logger.log(totalr / i, stdr)\n",
    "        logger.log_frames(frames)\n",
    "        frames = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/gym/envs/registration.py:14: DeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "# test density fcn \n",
    "env = gym.make('MontezumaRevenge-v0')\n",
    "# default params taken from homework\n",
    "d_graph_args = {\n",
    "    'ob_dim': env.observation_space.shape,\n",
    "    'learning_rate': 5e-3,\n",
    "    'z_size': 32,\n",
    "    'kl_weight': 1e-2,\n",
    "    'conv_depth': 5,\n",
    "    'hid_size': 32,\n",
    "    'n_hidden': 4\n",
    "}\n",
    "p_graph_args = {\n",
    "    'ob_dim': env.observation_space.shape,\n",
    "    'act_dim': env.action_space.n,\n",
    "    'clip_range': 0.2,\n",
    "    'conv_depth': 5,\n",
    "    'filter_size': 32,\n",
    "    'learning_rate': 5e-3,\n",
    "    'num_target_updates': 10,\n",
    "    'num_grad_steps_per_target_update': 10\n",
    "}\n",
    "adv_args = {\n",
    "    'gamma': 0.9999999\n",
    "}\n",
    "\n",
    "# build graph\n",
    "# models\n",
    "policy = Policy(p_graph_args, adv_args)\n",
    "density_model = DensityModel(d_graph_args)\n",
    "# utils\n",
    "replay_buffer = MasterBuffer()\n",
    "logger = Logger()\n",
    "\n",
    "# agent which will do all the work\n",
    "agent = Agent(env, policy, density_model, replay_buffer, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to train...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-5984d833ec2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mitr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'completed itr {}...\\r'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-2c3aa437b007>\u001b[0m in \u001b[0;36msample_env\u001b[0;34m(self, num_samples)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mnxt_ob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-2c3aa437b007>\u001b[0m in \u001b[0;36mchoose_action\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# Greedy action for now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-9852730e6081>\u001b[0m in \u001b[0;36mget_best_action\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_best_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         act = sess.run(self.greedy_action, feed_dict={\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         })[0]\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# tf.reset_default_graph()\n",
    "tf_config = tf.ConfigProto(inter_op_parallelism_threads=1, intra_op_parallelism_threads=1)\n",
    "# tf_config.gpu_options.allow_growth = True\n",
    "with tf.Session(config=tf_config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    agent.set_session(sess)\n",
    "    \n",
    "    n_iter = 1\n",
    "    num_samples = 1\n",
    "    batch_sizes = [1]\n",
    "    render_n = 1\n",
    "    \n",
    "    print('starting to train...')\n",
    "    for bs in batch_sizes:\n",
    "        logger.set_tag('bs: '+str(bs))\n",
    "        replay_buffer.flush()\n",
    "        \n",
    "        for itr in range(n_iter):\n",
    "            agent.sample_env(num_samples)\n",
    "            agent.train(batch_size=bs)\n",
    "            print('completed itr {}...\\r'.format(str(itr)))\n",
    "            if itr % render_n == 0 and itr != 0:\n",
    "                agent.test(1, render=True)\n",
    "                print('render complete...')\n",
    "        logger.package_results()\n",
    "        logger.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for results in logger.results:\n",
    "    totalr, stdr = results[0], results[1]\n",
    "    plt.plot(totalr)\n",
    "plt.title('AvgTotalReward vs Train Iteration')\n",
    "plt.xlabel('TrainIteration')\n",
    "plt.ylabel('AvgTotalReward')\n",
    "plt.legend(logger.tags, loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- transport code to GPU environment to test density function\n",
    "- split density training into seperate function <b>later</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logger' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-261ed90c3ee3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Iteration:'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mrender_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# just update the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'logger' is not defined"
     ]
    }
   ],
   "source": [
    "for frames, tag in zip(logger.frame_logs, logger.frame_tags):\n",
    "    img = plt.imshow(frames[0])\n",
    "    plt.title('Iteration:' + str(tag*render_n))\n",
    "    for frame in frames:\n",
    "        img.set_data(frame) # just update the data\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
